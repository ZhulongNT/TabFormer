{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabFormer: Tabular BERT for Credit Card Transaction Modeling\n",
    "\n",
    "This notebook trains a hierarchical BERT model on credit card transaction data using the TabFormer architecture.\n",
    "Designed to run in Kaggle environment with GPU acceleration.\n",
    "\n",
    "## Overview\n",
    "- **Model**: Hierarchical Tabular BERT with field-wise cross-entropy\n",
    "- **Task**: Masked Language Modeling on transaction sequences\n",
    "- **Data**: Synthetic credit card transactions (24M records)\n",
    "- **Hardware**: GPU-accelerated training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Kaggle environment\n",
    "# Uninstall existing torch packages to avoid CUDA version conflicts\n",
    "!pip uninstall -y torch torchvision torchaudio\n",
    "\n",
    "# Install PyTorch with CUDA 11.8 support (all from same index)\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install transformers with torch extras and other dependencies\n",
    "!pip install -q \"transformers[torch]>=4.30.0\" scikit-learn>=1.0.0 pandas>=1.3.0 numpy>=1.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertConfig,\n",
    "    BertModel,\n",
    "    BertForMaskedLM,\n",
    "    PreTrainedModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Display GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'data_root': './data/credit_card/',\n",
    "    'data_fname': 'card_transaction.v1',\n",
    "    'output_dir': './output',\n",
    "    'vocab_dir': './output',\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    'field_hidden_size': 64,  # Reduced for faster training\n",
    "    'mlm_prob': 0.15,\n",
    "    \n",
    "    # Data parameters\n",
    "    'seq_len': 10,  # Sequence length (number of transactions)\n",
    "    'stride': 5,  # Stride for sliding window\n",
    "    'num_bins': 10,  # Number of bins for quantization\n",
    "    'nrows': 100000,  # Number of rows to use (None for all data)\n",
    "    'user_ids': None,  # Filter by user IDs (None for all users)\n",
    "    \n",
    "    # Training parameters\n",
    "    'num_train_epochs': 3,\n",
    "    'batch_size': 32,\n",
    "    'save_steps': 500,\n",
    "    'logging_steps': 100,\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['vocab_dir'], exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "display(HTML(f\"<h3>Configuration Set</h3><p>Using {CONFIG['nrows']} rows for training</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary Class\n",
    "\n",
    "Manages the vocabulary for tabular data with field-aware tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    \"\"\"Dictionary that allows attribute-style access\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Vocabulary for tabular data with field-aware tokenization\"\"\"\n",
    "    \n",
    "    def __init__(self, adap_thres=10000, target_column_name=\"Is Fraud?\"):\n",
    "        # Special tokens\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.sep_token = \"[SEP]\"\n",
    "        self.pad_token = \"[PAD]\"\n",
    "        self.cls_token = \"[CLS]\"\n",
    "        self.mask_token = \"[MASK]\"\n",
    "        self.bos_token = \"[BOS]\"\n",
    "        self.eos_token = \"[EOS]\"\n",
    "        \n",
    "        self.adap_thres = adap_thres\n",
    "        self.adap_sm_cols = set()\n",
    "        \n",
    "        self.target_column_name = target_column_name\n",
    "        self.special_field_tag = \"SPECIAL\"\n",
    "        \n",
    "        self.special_tokens = [self.unk_token, self.sep_token, self.pad_token,\n",
    "                               self.cls_token, self.mask_token, self.bos_token, self.eos_token]\n",
    "        \n",
    "        self.token2id = OrderedDict()  # {field: {token: [global_id, local_id]}, ...}\n",
    "        self.id2token = OrderedDict()  # {global_id: [token, field, local_id]}\n",
    "        self.field_keys = OrderedDict()\n",
    "        self.token2id[self.special_field_tag] = OrderedDict()\n",
    "        \n",
    "        self.filename = ''\n",
    "        \n",
    "        # Initialize special tokens\n",
    "        for token in self.special_tokens:\n",
    "            global_id = len(self.id2token)\n",
    "            local_id = len(self.token2id[self.special_field_tag])\n",
    "            self.token2id[self.special_field_tag][token] = [global_id, local_id]\n",
    "            self.id2token[global_id] = [token, self.special_field_tag, local_id]\n",
    "    \n",
    "    def set_field_keys(self, keys):\n",
    "        \"\"\"Initialize field keys\"\"\"\n",
    "        for key in keys:\n",
    "            self.token2id[key] = OrderedDict()\n",
    "            global_id = len(self.id2token)\n",
    "            local_id = len(self.token2id[key])\n",
    "            self.token2id[key][self.sep_token] = [global_id, local_id]\n",
    "            self.id2token[global_id] = [self.sep_token, key, local_id]\n",
    "            self.field_keys[key] = global_id\n",
    "    \n",
    "    def set_id(self, token, field_name, return_local=False):\n",
    "        \"\"\"Set token ID for a field\"\"\"\n",
    "        if token not in self.token2id[field_name]:\n",
    "            global_id = len(self.id2token)\n",
    "            local_id = len(self.token2id[field_name])\n",
    "            self.token2id[field_name][token] = [global_id, local_id]\n",
    "            self.id2token[global_id] = [token, field_name, local_id]\n",
    "        else:\n",
    "            global_id, local_id = self.token2id[field_name][token]\n",
    "        \n",
    "        return local_id if return_local else global_id\n",
    "    \n",
    "    def get_id(self, token, field_name=\"\", special_token=False, return_local=False):\n",
    "        \"\"\"Get token ID\"\"\"\n",
    "        if special_token:\n",
    "            field_name = self.special_field_tag\n",
    "        \n",
    "        if token in self.token2id[field_name]:\n",
    "            global_id, local_id = self.token2id[field_name][token]\n",
    "        else:\n",
    "            raise Exception(f\"Token {token} not found in field: {field_name}\")\n",
    "        \n",
    "        return local_id if return_local else global_id\n",
    "    \n",
    "    def get_special_tokens(self):\n",
    "        \"\"\"Return special tokens as AttrDict\"\"\"\n",
    "        return AttrDict(\n",
    "            unk_token=self.unk_token,\n",
    "            sep_token=self.sep_token,\n",
    "            pad_token=self.pad_token,\n",
    "            cls_token=self.cls_token,\n",
    "            mask_token=self.mask_token,\n",
    "            bos_token=self.bos_token,\n",
    "            eos_token=self.eos_token\n",
    "        )\n",
    "    \n",
    "    def get_from_local_ids(self, field, local_ids):\n",
    "        \"\"\"Convert local IDs to global IDs\"\"\"\n",
    "        return [self.token2id[field][token][0] for token, ids in self.token2id[field].items() if ids[1] in local_ids]\n",
    "    \n",
    "    def save_vocab(self, file_name):\n",
    "        \"\"\"Save vocabulary to file\"\"\"\n",
    "        self.filename = file_name\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        logger.info(f\"Vocabulary saved to {file_name}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.id2token)\n",
    "\n",
    "print(\"\u2713 Vocabulary class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transaction Dataset Class\n",
    "\n",
    "Processes credit card transaction data into sequences for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionDataset(Dataset):\n",
    "    \"\"\"Credit card transaction dataset for sequence modeling\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 mlm=True,\n",
    "                 user_ids=None,\n",
    "                 seq_len=10,\n",
    "                 num_bins=10,\n",
    "                 cached=False,\n",
    "                 root=\"./data/card/\",\n",
    "                 fname=\"card_trans\",\n",
    "                 vocab_dir=\"checkpoints\",\n",
    "                 fextension=\"\",\n",
    "                 nrows=None,\n",
    "                 flatten=False,\n",
    "                 stride=5,\n",
    "                 adap_thres=10**8,\n",
    "                 return_labels=False,\n",
    "                 skip_user=False):\n",
    "        \n",
    "        self.root = root\n",
    "        self.fname = fname\n",
    "        self.nrows = nrows\n",
    "        self.fextension = f'_{fextension}' if fextension else ''\n",
    "        self.cached = cached\n",
    "        self.user_ids = user_ids\n",
    "        self.return_labels = return_labels\n",
    "        self.skip_user = skip_user\n",
    "        \n",
    "        self.mlm = mlm\n",
    "        self.trans_stride = stride\n",
    "        self.flatten = flatten\n",
    "        \n",
    "        self.vocab = Vocabulary(adap_thres)\n",
    "        self.seq_len = seq_len\n",
    "        self.encoder_fit = {}\n",
    "        \n",
    "        self.trans_table = None\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.window_label = []\n",
    "        \n",
    "        self.ncols = None\n",
    "        self.num_bins = num_bins\n",
    "        \n",
    "        # Process data\n",
    "        self.encode_data()\n",
    "        self.init_vocab()\n",
    "        self.prepare_samples()\n",
    "        self.save_vocab(vocab_dir)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.flatten:\n",
    "            return_data = torch.tensor(self.data[index], dtype=torch.long)\n",
    "        else:\n",
    "            return_data = torch.tensor(self.data[index], dtype=torch.long).reshape(self.seq_len, -1)\n",
    "        \n",
    "        if self.return_labels:\n",
    "            return_data = (return_data, torch.tensor(self.labels[index], dtype=torch.long))\n",
    "        \n",
    "        return return_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def save_vocab(self, vocab_dir):\n",
    "        \"\"\"Save vocabulary\"\"\"\n",
    "        file_name = os.path.join(vocab_dir, f'vocab{self.fextension}.nb')\n",
    "        self.vocab.save_vocab(file_name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def label_fit_transform(column, enc_type=\"label\"):\n",
    "        \"\"\"Fit and transform column using encoder\"\"\"\n",
    "        if enc_type == \"label\":\n",
    "            mfit = LabelEncoder()\n",
    "        else:\n",
    "            mfit = MinMaxScaler()\n",
    "        mfit.fit(column)\n",
    "        return mfit, mfit.transform(column)\n",
    "    \n",
    "    @staticmethod\n",
    "    def nanNone(x):\n",
    "        \"\"\"Replace NaN with 'None'\"\"\"\n",
    "        return x.fillna('None')\n",
    "    \n",
    "    @staticmethod\n",
    "    def nanZero(x):\n",
    "        \"\"\"Replace NaN with 0\"\"\"\n",
    "        return x.fillna(0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def fraudEncoder(x):\n",
    "        \"\"\"Encode fraud labels\"\"\"\n",
    "        return x.map(lambda val: 1 if val == 'Yes' else 0)\n",
    "    \n",
    "    def _quantize(self, inputs, bin_edges):\n",
    "        \"\"\"Quantize continuous values\"\"\"\n",
    "        out_tokens = []\n",
    "        for val in inputs:\n",
    "            out_tokens.append(np.abs(bin_edges - val).argmin())\n",
    "        return out_tokens\n",
    "    \n",
    "    def encode_data(self):\n",
    "        \"\"\"Encode transaction data\"\"\"\n",
    "        data_file = os.path.join(self.root, f\"{self.fname}.csv\")\n",
    "        \n",
    "        logger.info(f\"Reading data from {data_file}\")\n",
    "        data = pd.read_csv(data_file, nrows=self.nrows)\n",
    "        logger.info(f\"Loaded {len(data)} transactions\")\n",
    "        \n",
    "        # Handle NaN values\n",
    "        data['Errors?'] = self.nanNone(data['Errors?'])\n",
    "        data['Is Fraud?'] = self.fraudEncoder(data['Is Fraud?'])\n",
    "        data['Zip'] = self.nanZero(data['Zip'])\n",
    "        data['Merchant State'] = self.nanNone(data['Merchant State'])\n",
    "        \n",
    "        # Remove dollar sign from Amount\n",
    "        data['Amount'] = data['Amount'].apply(lambda x: float(x.replace('$', '')))\n",
    "        \n",
    "        # Encode categorical columns\n",
    "        cat_cols = ['Merchant Name', 'Merchant City', 'Merchant State', 'Zip', \n",
    "                   'MCC', 'Errors?', 'Use Chip', 'Year', 'Month', 'Day']\n",
    "        \n",
    "        for col in cat_cols:\n",
    "            self.encoder_fit[col], data[col] = self.label_fit_transform(data[col].astype(str))\n",
    "        \n",
    "        # Quantize Amount\n",
    "        min_val, max_val = data['Amount'].min(), data['Amount'].max()\n",
    "        bin_edges = np.linspace(min_val, max_val, self.num_bins)\n",
    "        data['Amount'] = self._quantize(data['Amount'].values, bin_edges)\n",
    "        \n",
    "        # Convert Time to numeric\n",
    "        data['Time'] = data['Time'].apply(lambda x: int(x.split(':')[0]) * 60 + int(x.split(':')[1]))\n",
    "        self.encoder_fit['Time'], data['Time'] = self.label_fit_transform(data['Time'])\n",
    "        \n",
    "        if not self.skip_user:\n",
    "            self.encoder_fit['User'], data['User'] = self.label_fit_transform(data['User'])\n",
    "        \n",
    "        self.encoder_fit['Card'], data['Card'] = self.label_fit_transform(data['Card'])\n",
    "        \n",
    "        self.trans_table = data\n",
    "        logger.info(\"Data encoding completed\")\n",
    "    \n",
    "    def init_vocab(self):\n",
    "        \"\"\"Initialize vocabulary from encoded data\"\"\"\n",
    "        columns = list(self.trans_table.columns)\n",
    "        \n",
    "        # Remove label column\n",
    "        columns.remove('Is Fraud?')\n",
    "        \n",
    "        if self.skip_user:\n",
    "            columns.remove('User')\n",
    "        \n",
    "        self.vocab.set_field_keys(columns)\n",
    "        \n",
    "        # Build vocabulary\n",
    "        for column in tqdm(columns, desc=\"Building vocabulary\"):\n",
    "            for val in self.trans_table[column].unique():\n",
    "                token = str(val)\n",
    "                self.vocab.set_id(token, column)\n",
    "        \n",
    "        logger.info(f\"Vocabulary size: {len(self.vocab)}\")\n",
    "    \n",
    "    def user_level_data(self):\n",
    "        \"\"\"Organize data by user\"\"\"\n",
    "        if self.user_ids:\n",
    "            user_ids = [int(u) for u in self.user_ids]\n",
    "        else:\n",
    "            user_ids = self.trans_table['User'].unique()\n",
    "        \n",
    "        user_data = {}\n",
    "        for user_id in user_ids:\n",
    "            user_data[user_id] = self.trans_table[self.trans_table['User'] == user_id]\n",
    "        \n",
    "        return user_data\n",
    "    \n",
    "    def prepare_samples(self):\n",
    "        \"\"\"Prepare training samples from transactions\"\"\"\n",
    "        logger.info(\"Preparing samples...\")\n",
    "        \n",
    "        columns = list(self.trans_table.columns)\n",
    "        columns.remove('Is Fraud?')\n",
    "        if self.skip_user:\n",
    "            columns.remove('User')\n",
    "        \n",
    "        user_data = self.user_level_data()\n",
    "        \n",
    "        cls_token = self.vocab.get_id(self.vocab.cls_token, special_token=True)\n",
    "        sep_token = self.vocab.get_id(self.vocab.sep_token, special_token=True)\n",
    "        \n",
    "        for user_id, user_trans in tqdm(user_data.items(), desc=\"Processing users\"):\n",
    "            user_vocab_ids = []\n",
    "            user_labels = []\n",
    "            \n",
    "            for _, trans in user_trans.iterrows():\n",
    "                trans_ids = []\n",
    "                \n",
    "                if not self.flatten:\n",
    "                    trans_ids.append(cls_token)\n",
    "                \n",
    "                for column in columns:\n",
    "                    token = str(trans[column])\n",
    "                    token_id = self.vocab.get_id(token, column)\n",
    "                    trans_ids.append(token_id)\n",
    "                    \n",
    "                    if not self.flatten:\n",
    "                        trans_ids.append(sep_token)\n",
    "                \n",
    "                user_vocab_ids.append(trans_ids)\n",
    "                user_labels.append(trans['Is Fraud?'])\n",
    "            \n",
    "            # Create sliding windows\n",
    "            for idx in range(0, len(user_vocab_ids) - self.seq_len + 1, self.trans_stride):\n",
    "                ids = user_vocab_ids[idx:(idx + self.seq_len)]\n",
    "                ids = [item for sublist in ids for item in sublist]  # Flatten\n",
    "                self.data.append(ids)\n",
    "            \n",
    "            for jdx in range(0, len(user_labels) - self.seq_len + 1, self.trans_stride):\n",
    "                ids = user_labels[jdx:(jdx + self.seq_len)]\n",
    "                self.labels.append(ids)\n",
    "                \n",
    "                fraud = 1 if sum(ids) > 0 else 0\n",
    "                self.window_label.append(fraud)\n",
    "        \n",
    "        assert len(self.data) == len(self.labels)\n",
    "        \n",
    "        # Calculate ncols (number of columns)\n",
    "        self.ncols = len(self.vocab.field_keys) - 2 + (1 if self.mlm else 0)\n",
    "        \n",
    "        logger.info(f\"Prepared {len(self.data)} samples\")\n",
    "        logger.info(f\"Number of columns: {self.ncols}\")\n",
    "\n",
    "print(\"\u2713 TransactionDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "display(HTML(\"<h3>Loading Transaction Data</h3>\"))\n",
    "\n",
    "dataset = TransactionDataset(\n",
    "    root=CONFIG['data_root'],\n",
    "    fname=CONFIG['data_fname'],\n",
    "    vocab_dir=CONFIG['vocab_dir'],\n",
    "    nrows=CONFIG['nrows'],\n",
    "    user_ids=CONFIG['user_ids'],\n",
    "    mlm=True,\n",
    "    cached=False,\n",
    "    stride=CONFIG['stride'],\n",
    "    flatten=False,  # Use hierarchical model\n",
    "    return_labels=False,\n",
    "    skip_user=False,\n",
    "    seq_len=CONFIG['seq_len'],\n",
    "    num_bins=CONFIG['num_bins']\n",
    ")\n",
    "\n",
    "vocab = dataset.vocab\n",
    "custom_special_tokens = vocab.get_special_tokens()\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"  Total samples: {len(dataset):,}\")\n",
    "print(f\"  Vocabulary size: {len(vocab):,}\")\n",
    "print(f\"  Number of columns: {dataset.ncols}\")\n",
    "print(f\"  Sequence length: {CONFIG['seq_len']}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split_dataset(dataset, lengths, random_seed=20200706):\n",
    "    \"\"\"Split dataset with reproducible randomness\"\"\"\n",
    "    # Save state\n",
    "    state = {\n",
    "        'python_state': random.getstate(),\n",
    "        'numpy_state': np.random.get_state(),\n",
    "        'torch_state': torch.get_rng_state(),\n",
    "        'cuda_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None\n",
    "    }\n",
    "    \n",
    "    # Set seed\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "    \n",
    "    # Split\n",
    "    train_dataset, eval_dataset, test_dataset = torch.utils.data.random_split(dataset, lengths)\n",
    "    \n",
    "    # Restore state\n",
    "    random.setstate(state['python_state'])\n",
    "    np.random.set_state(state['numpy_state'])\n",
    "    torch.set_rng_state(state['torch_state'])\n",
    "    if torch.cuda.is_available() and state['cuda_state'] is not None:\n",
    "        torch.cuda.set_rng_state(state['cuda_state'])\n",
    "    \n",
    "    return train_dataset, eval_dataset, test_dataset\n",
    "\n",
    "# Split dataset: 60% train, 20% val, 20% test\n",
    "totalN = len(dataset)\n",
    "trainN = int(0.6 * totalN)\n",
    "valtestN = totalN - trainN\n",
    "valN = int(valtestN * 0.5)\n",
    "testN = valtestN - valN\n",
    "\n",
    "lengths = [trainN, valN, testN]\n",
    "train_dataset, eval_dataset, test_dataset = random_split_dataset(dataset, lengths)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Dataset Split:\")\n",
    "print(f\"  Train: {trainN:,} ({trainN/totalN:.1%})\")\n",
    "print(f\"  Val:   {valN:,} ({valN/totalN:.1%})\")\n",
    "print(f\"  Test:  {testN:,} ({testN/totalN:.1%})\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TabFormer Model Architecture\n",
    "\n",
    "Hierarchical BERT model with field embeddings for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field Embeddings for hierarchical model\n",
    "class TabFormerEmbeddings(nn.Module):\n",
    "    \"\"\"Hierarchical embeddings for tabular data\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.field_hidden_size, \n",
    "                                           padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, \n",
    "                                               config.field_hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.field_hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "        \n",
    "        position_ids = self.position_ids[:, :seq_length]\n",
    "        \n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        embeddings = inputs_embeds + position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Custom BERT Configuration\n",
    "class TabFormerBertConfig(BertConfig):\n",
    "    \"\"\"Configuration for TabFormer BERT\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 ncols=12,\n",
    "                 field_hidden_size=64,\n",
    "                 flatten=False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ncols = ncols\n",
    "        self.field_hidden_size = field_hidden_size\n",
    "        self.flatten = flatten\n",
    "\n",
    "\n",
    "# TabFormer BERT for Masked LM\n",
    "class TabFormerBertForMaskedLM(BertForMaskedLM):\n",
    "    \"\"\"BERT model with field-wise cross-entropy\"\"\"\n",
    "    \n",
    "    def __init__(self, config, vocab):\n",
    "        super().__init__(config)\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def forward(self, input_ids=None, inputs_embeds=None, masked_lm_labels=None, **kwargs):\n",
    "        if inputs_embeds is None and input_ids is not None:\n",
    "            outputs = super().forward(input_ids=input_ids, labels=masked_lm_labels, **kwargs)\n",
    "        else:\n",
    "            outputs = super().forward(inputs_embeds=inputs_embeds, labels=masked_lm_labels, **kwargs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Hierarchical TabFormer Model\n",
    "class TabFormerHierarchicalLM(PreTrainedModel):\n",
    "    \"\"\"Hierarchical TabFormer with field embeddings\"\"\"\n",
    "    base_model_prefix = \"bert\"\n",
    "    \n",
    "    def __init__(self, config, vocab):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.tab_embeddings = TabFormerEmbeddings(config)\n",
    "        self.tb_model = TabFormerBertForMaskedLM(config, vocab)\n",
    "    \n",
    "    def forward(self, input_ids, **input_args):\n",
    "        inputs_embeds = self.tab_embeddings(input_ids)\n",
    "        return self.tb_model(inputs_embeds=inputs_embeds, **input_args)\n",
    "\n",
    "\n",
    "# TabFormer BERT Language Model Wrapper\n",
    "class TabFormerBertLM:\n",
    "    \"\"\"TabFormer BERT Language Model\"\"\"\n",
    "    \n",
    "    def __init__(self, special_tokens, vocab, field_ce=False, flatten=False, \n",
    "                 ncols=None, field_hidden_size=768):\n",
    "        self.ncols = ncols\n",
    "        self.vocab = vocab\n",
    "        vocab_file = self.vocab.filename\n",
    "        \n",
    "        hidden_size = field_hidden_size if flatten else (field_hidden_size * self.ncols)\n",
    "        \n",
    "        self.config = TabFormerBertConfig(\n",
    "            vocab_size=len(self.vocab),\n",
    "            ncols=self.ncols,\n",
    "            hidden_size=hidden_size,\n",
    "            field_hidden_size=field_hidden_size,\n",
    "            flatten=flatten,\n",
    "            num_attention_heads=self.ncols\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = BertTokenizer(\n",
    "            vocab_file,\n",
    "            do_lower_case=False,\n",
    "            **special_tokens\n",
    "        )\n",
    "        \n",
    "        self.model = self.get_model(field_ce, flatten)\n",
    "    \n",
    "    def get_model(self, field_ce, flatten):\n",
    "        \"\"\"Get appropriate model based on configuration\"\"\"\n",
    "        if flatten and not field_ce:\n",
    "            # Flattened vanilla BERT\n",
    "            model = BertForMaskedLM(self.config)\n",
    "        elif flatten and field_ce:\n",
    "            # Flattened field CE BERT\n",
    "            model = TabFormerBertForMaskedLM(self.config, self.vocab)\n",
    "        else:\n",
    "            # Hierarchical field CE BERT\n",
    "            model = TabFormerHierarchicalLM(self.config, self.vocab)\n",
    "        \n",
    "        return model\n",
    "\n",
    "print(\"\u2713 TabFormer model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<h3>Initializing TabFormer BERT Model</h3>\"))\n",
    "\n",
    "tab_net = TabFormerBertLM(\n",
    "    custom_special_tokens,\n",
    "    vocab=vocab,\n",
    "    field_ce=True,  # Use field-wise cross-entropy\n",
    "    flatten=False,  # Use hierarchical model\n",
    "    ncols=dataset.ncols,\n",
    "    field_hidden_size=CONFIG['field_hidden_size']\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  Type: {tab_net.model.__class__.__name__}\")\n",
    "print(f\"  Vocabulary size: {len(vocab):,}\")\n",
    "print(f\"  Hidden size: {tab_net.config.hidden_size}\")\n",
    "print(f\"  Field hidden size: {tab_net.config.field_hidden_size}\")\n",
    "print(f\"  Number of attention heads: {tab_net.config.num_attention_heads}\")\n",
    "print(f\"  Number of layers: {tab_net.config.num_hidden_layers}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in tab_net.model.parameters())\n",
    "trainable_params = sum(p.numel() for p in tab_net.model.parameters() if p.requires_grad)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Collator for MLM\n",
    "\n",
    "Custom data collator for hierarchical masked language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n",
    "    \"\"\"Data collator for hierarchical tabular data\"\"\"\n",
    "    \n",
    "    def __call__(self, examples: List[Union[List[int], torch.Tensor, Dict[str, torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self._tensorize_batch(examples)\n",
    "        sz = batch.shape\n",
    "        \n",
    "        if self.mlm:\n",
    "            # Flatten for masking\n",
    "            batch = batch.view(sz[0], -1)\n",
    "            inputs, labels = self.mask_tokens(batch)\n",
    "            # Reshape back to hierarchical format\n",
    "            return {\"input_ids\": inputs.view(sz), \"masked_lm_labels\": labels.view(sz)}\n",
    "        else:\n",
    "            labels = batch.clone().detach()\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            return {\"input_ids\": batch, \"labels\": labels}\n",
    "    \n",
    "    def mask_tokens(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Prepare masked tokens: 80% MASK, 10% random, 10% original\"\"\"\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\"This tokenizer does not have a mask token\")\n",
    "        \n",
    "        labels = inputs.clone()\n",
    "        \n",
    "        # Sample tokens for MLM (15% by default)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) \n",
    "            for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        \n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "        \n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # Only compute loss on masked tokens\n",
    "        \n",
    "        # 80% of the time, replace with [MASK]\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        \n",
    "        # 10% of the time, replace with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "        \n",
    "        # 10% of the time, keep the token unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "# Create data collator\n",
    "data_collator = TransDataCollatorForLanguageModeling(\n",
    "    tokenizer=tab_net.tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=CONFIG['mlm_prob']\n",
    ")\n",
    "\n",
    "print(\"\u2713 Data collator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    num_train_epochs=CONFIG['num_train_epochs'],\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "    logging_dir=os.path.join(CONFIG['output_dir'], 'logs'),\n",
    "    logging_steps=CONFIG['logging_steps'],\n",
    "    save_steps=CONFIG['save_steps'],\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    # GPU optimization\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision on GPU\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Epochs: {CONFIG['num_train_epochs']}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Mixed precision (FP16): {training_args.fp16}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=tab_net.model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "print(\"\u2713 Trainer initialized\")\n",
    "display(HTML(\"<h3>Ready to Train!</h3>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train the Model\n",
    "\n",
    "Start training the TabFormer model on credit card transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<h2>\ud83d\ude80 Training Started</h2>\"))\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "display(HTML(\"<h2>\u2705 Training Complete!</h2>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = os.path.join(CONFIG['output_dir'], 'final_model')\n",
    "trainer.save_model(final_model_path)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Model saved to: {final_model_path}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "display(HTML(f\"<h3>\u2713 Model saved successfully</h3><p>Location: {final_model_path}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Dataset: Credit Card Transactions\")\n",
    "print(f\"Model: Hierarchical TabFormer BERT\")\n",
    "print(f\"Training samples: {trainN:,}\")\n",
    "print(f\"Validation samples: {valN:,}\")\n",
    "print(f\"Epochs: {CONFIG['num_train_epochs']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Field hidden size: {CONFIG['field_hidden_size']}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "## Next Steps\n",
    "\n",
    "1. **Evaluate the model**: Use the trained model for inference on test data\n",
    "2. **Fine-tune**: Adjust hyperparameters for better performance\n",
    "3. **Deploy**: Export the model for production use\n",
    "4. **Analyze**: Examine attention patterns and learned representations\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}