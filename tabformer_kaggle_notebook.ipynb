{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TabFormer: Tabular Transformer for Kaggle\n",
        "\n",
        "This notebook implements **TabFormer** - applying Transformer models to tabular transaction data.\n",
        "\n",
        "**Features:**\n",
        "- BERT-based masked language modeling for tabular data\n",
        "- Field-wise cross-entropy loss\n",
        "- Hierarchical embeddings\n",
        "- CPU-optimized\n",
        "\n",
        "**Paths:**\n",
        "- Input: `/kaggle/input/`\n",
        "- Output: `/kaggle/working/`\n",
        "\n",
        "**Reference:** [TabFormer Paper](https://arxiv.org/abs/2010.11653)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install transformers with torch support\n",
        "!pip install -q transformers[torch] --upgrade\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Transformers: {transformers.__version__}\")\n",
        "print(f\"Pandas: {pd.__version__}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "print(f\"\\nDevice: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# Configure for CPU\n",
        "import os\n",
        "device = 'cpu'\n",
        "print(f\"Using: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import All Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import os\n",
        "from os import path, makedirs\n",
        "import logging\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Tuple, Union\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.nn import CrossEntropyLoss, AdaptiveLogSoftmaxWithLoss\n",
        "from torch.nn.functional import log_softmax\n",
        "\n",
        "# Transformers\n",
        "from transformers import (\n",
        "    BertTokenizer, BertForMaskedLM, BertConfig,\n",
        "    GPT2Config, GPT2LMHeadModel,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "from transformers.models.bert.modeling_bert import ACT2FN\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n",
        "\n",
        "# IPython display\n",
        "from IPython.display import display, HTML, Markdown\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "log = logger\n",
        "\n",
        "print(\"\u2713 All libraries imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n\nclass ddict(object):\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n\n\ndef random_split_dataset(dataset, lengths, random_seed=20200706):\n    # state snapshot\n    state = {}\n    state['seeds'] = {\n        'python_state': random.getstate(),\n        'numpy_state': np.random.get_state(),\n        'torch_state': torch.get_rng_state(),\n        'cuda_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None\n    }\n\n    # seed\n    random.seed(random_seed)  # python\n    np.random.seed(random_seed)  # numpy\n    torch.manual_seed(random_seed)  # torch\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(random_seed)  # torch.cuda\n\n    train_dataset, eval_dataset, test_dataset = torch.utils.data.dataset.random_split(dataset, lengths)\n\n    # reinstate state\n    random.setstate(state['seeds']['python_state'])\n    np.random.set_state(state['seeds']['numpy_state'])\n    torch.set_rng_state(state['seeds']['torch_state'])\n    if torch.cuda.is_available():\n        torch.cuda.set_rng_state(state['seeds']['cuda_state'])\n\n    return train_dataset, eval_dataset, test_dataset\n\n\ndef divide_chunks(l, n):\n    for i in range(0, len(l), n):\n        yield l[i:i + n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Vocabulary Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n\nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n\n\nclass Vocabulary:\n    def __init__(self, adap_thres=10000, target_column_name=\"Is Fraud?\"):\n        self.unk_token = \"[UNK]\"\n        self.sep_token = \"[SEP]\"\n        self.pad_token = \"[PAD]\"\n        self.cls_token = \"[CLS]\"\n        self.mask_token = \"[MASK]\"\n        self.bos_token = \"[BOS]\"\n        self.eos_token = \"[EOS]\"\n\n        self.adap_thres = adap_thres\n        self.adap_sm_cols = set()\n\n        self.target_column_name = target_column_name\n        self.special_field_tag = \"SPECIAL\"\n\n        self.special_tokens = [self.unk_token, self.sep_token, self.pad_token,\n                               self.cls_token, self.mask_token, self.bos_token, self.eos_token]\n\n        self.token2id = OrderedDict()  # {field: {token: id}, ...}\n        self.id2token = OrderedDict()  # {id : [token,field]}\n        self.field_keys = OrderedDict()\n        self.token2id[self.special_field_tag] = OrderedDict()\n\n        self.filename = ''  # this field is set in the `save_vocab` method\n\n        for token in self.special_tokens:\n            global_id = len(self.id2token)\n            local_id = len(self.token2id[self.special_field_tag])\n\n            self.token2id[self.special_field_tag][token] = [global_id, local_id]\n            self.id2token[global_id] = [token, self.special_field_tag, local_id]\n\n    def set_id(self, token, field_name, return_local=False):\n        global_id, local_id = None, None\n\n        if token not in self.token2id[field_name]:\n            global_id = len(self.id2token)\n            local_id = len(self.token2id[field_name])\n\n            self.token2id[field_name][token] = [global_id, local_id]\n            self.id2token[global_id] = [token, field_name, local_id]\n        else:\n            global_id, local_id = self.token2id[field_name][token]\n\n        if return_local:\n            return local_id\n\n        return global_id\n\n    def get_id(self, token, field_name=\"\", special_token=False, return_local=False):\n        global_id, local_id = None, None\n        if special_token:\n            field_name = self.special_field_tag\n\n        if token in self.token2id[field_name]:\n            global_id, local_id = self.token2id[field_name][token]\n\n        else:\n            raise Exception(f\"token {token} not found in field: {field_name}\")\n\n        if return_local:\n            return local_id\n\n        return global_id\n\n    def set_field_keys(self, keys):\n\n        for key in keys:\n            self.token2id[key] = OrderedDict()\n            self.field_keys[key] = None\n\n        self.field_keys[self.special_field_tag] = None  # retain the order of columns\n\n    def get_field_ids(self, field_name, return_local=False):\n        if field_name in self.token2id:\n            ids = self.token2id[field_name]\n        else:\n            raise Exception(f\"field name {field_name} is invalid.\")\n\n        selected_idx = 0\n        if return_local:\n            selected_idx = 1\n        return [ids[idx][selected_idx] for idx in ids]\n\n    def get_from_global_ids(self, global_ids, what_to_get='local_ids'):\n        device = global_ids.device\n\n        def map_global_ids_to_local_ids(gid):\n            return self.id2token[gid][2] if gid != -100 else -100\n\n        def map_global_ids_to_tokens(gid):\n            return f'{self.id2token[gid][1]}_{self.id2token[gid][0]}' if gid != -100 else '-'\n\n        if what_to_get == 'local_ids':\n            return global_ids.cpu().apply_(map_global_ids_to_local_ids).to(device)\n        elif what_to_get == 'tokens':\n            vectorized_token_map = np.vectorize(map_global_ids_to_tokens)\n            new_array_for_tokens = global_ids.detach().clone().cpu().numpy()\n            return vectorized_token_map(new_array_for_tokens)\n        else:\n            raise ValueError(\"Only 'local_ids' or 'tokens' can be passed as value of the 'what_to_get' parameter.\")\n\n    def save_vocab(self, fname):\n        self.filename = fname\n        with open(fname, \"w\") as fout:\n            for idx in self.id2token:\n                token, field, _ = self.id2token[idx]\n                token = \"%s_%s\" % (field, token)\n                fout.write(\"%s\\n\" % token)\n\n    def get_field_keys(self, remove_target=True, ignore_special=False):\n        keys = list(self.field_keys.keys())\n\n        if remove_target and self.target_column_name in keys:\n            keys.remove(self.target_column_name)\n        if ignore_special:\n            keys.remove(self.special_field_tag)\n        return keys\n\n    def get_special_tokens(self):\n        special_tokens_map = {}\n        # TODO : remove the dependency of re-initializing here. retrieve from field_key = SPECIAL\n        keys = [\"unk_token\", \"sep_token\", \"pad_token\", \"cls_token\", \"mask_token\", \"bos_token\", \"eos_token\"]\n        for key, token in zip(keys, self.special_tokens):\n            token = \"%s_%s\" % (self.special_field_tag, token)\n            special_tokens_map[key] = token\n\n        return AttrDict(special_tokens_map)\n\n    def __len__(self):\n        return len(self.id2token)\n\n    def __str__(self):\n        str_ = 'vocab: [{} tokens]  [field_keys={}]'.format(len(self), self.field_keys)\n        return str_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Custom Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n\nclass CustomAdaptiveLogSoftmax(AdaptiveLogSoftmaxWithLoss):\n    def __init__(\n            self, ignore_index=-100,\n            **kwargs\n    ) -> None:\n        super().__init__(**kwargs)\n        self.ignore_index = ignore_index\n\n    def forward(self, input: Tensor, target: Tensor):\n        if input.size(0) != target.size(0):\n            raise RuntimeError('Input and target should have the same size '\n                               'in the batch dimension.')\n\n        '''\n            handles ignore index = -100;\n            removes all targets which are masked from input and target\n        '''\n        consider_indices = (target != self.ignore_index)\n        input = input[consider_indices, :]\n        target = target[consider_indices]\n\n        used_rows = 0\n        batch_size = target.size(0)\n\n        output = input.new_zeros(batch_size)\n        gather_inds = target.new_empty(batch_size)\n\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n\n            low_idx = cutoff_values[i]\n            high_idx = cutoff_values[i + 1]\n\n            target_mask = (target >= low_idx) & (target < high_idx)\n            row_indices = target_mask.nonzero().squeeze()\n\n            if row_indices.numel() == 0:\n                continue\n\n            if i == 0:\n                gather_inds.index_copy_(0, row_indices, target[target_mask])\n\n            else:\n                relative_target = target[target_mask] - low_idx\n                input_subset = input.index_select(0, row_indices)\n\n                cluster_output = self.tail[i - 1](input_subset)\n                cluster_index = self.shortlist_size + i - 1\n\n                gather_inds.index_fill_(0, row_indices, cluster_index)\n\n                cluster_logprob = log_softmax(cluster_output, dim=1)\n                local_logprob = cluster_logprob.gather(1, relative_target.unsqueeze(1))\n                output.index_copy_(0, row_indices, local_logprob.squeeze(1))\n\n            used_rows += row_indices.numel()\n\n        if used_rows != batch_size:\n            raise RuntimeError(\"Target values should be in [0, {}], \"\n                               \"but values in range [{}, {}] \"\n                               \"were found. \".format(self.n_classes - 1,\n                                                     target.min().item(),\n                                                     target.max().item()))\n\n        head_output = self.head(input)\n        head_logprob = log_softmax(head_output, dim=1)\n        output += head_logprob.gather(1, gather_inds.unsqueeze(1)).squeeze()\n        loss = (-output).mean()\n\n        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. TabFormer Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\nclass TabFormerTokenizer(PreTrainedTokenizer):\n    def __init__(\n        self,\n        unk_token=\"<|endoftext|>\",\n        bos_token=\"<|endoftext|>\",\n        eos_token=\"<|endoftext|>\",\n    ):\n\n        super().__init__(bos_token=bos_token, eos_token=eos_token, unk_token=unk_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Hierarchical Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n\nclass TabFormerConcatEmbeddings(nn.Module):\n    \"\"\"TabFormerConcatEmbeddings: Embeds tabular data of categorical variables\n\n        Notes: - All column entries must be integer indices in a vocabolary that is common across columns\n               - `sparse=True` in `nn.Embedding` speeds up gradient computation for large vocabs\n\n        Args:\n            config.ncols\n            config.vocab_size\n            config.hidden_size\n\n        Inputs:\n            - **input_ids** (batch, seq_len, ncols): tensor of batch of sequences of rows\n\n        Outputs:\n            - **output'**: (batch, seq_len, hidden_size): tensor of embedded rows\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.field_hidden_size,\n                                            padding_idx=getattr(config, 'pad_token_id', 0), sparse=False)\n        self.lin_proj = nn.Linear(config.field_hidden_size * config.ncols, config.hidden_size)\n\n        self.hidden_size = config.hidden_size\n        self.field_hidden_size = config.field_hidden_size\n\n    def forward(self, input_ids):\n        input_shape = input_ids.size()\n\n        embeds_sz = list(input_shape[:-1]) + [input_shape[-1] * self.field_hidden_size]\n        inputs_embeds = self.lin_proj(self.word_embeddings(input_ids).view(embeds_sz))\n\n        return inputs_embeds\n\n\nclass TabFormerEmbeddings(nn.Module):\n    \"\"\"TabFormerEmbeddings: Embeds tabular data of categorical variables\n\n        Notes: - All column entries must be integer indices in a vocabolary that is common across columns\n\n        Args:\n            config.ncols\n            config.num_layers (int): Number of transformer layers\n            config.vocab_size\n            config.hidden_size\n            config.field_hidden_size\n\n        Inputs:\n            - **input** (batch, seq_len, ncols): tensor of batch of sequences of rows\n\n        Outputs:\n            - **output**: (batch, seq_len, hidden_size): tensor of embedded rows\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n\n        if not hasattr(config, 'num_layers'):\n            config.num_layers = 1\n        if not hasattr(config, 'nhead'):\n            config.nhead = 8\n\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.field_hidden_size,\n                                            padding_idx=getattr(config, 'pad_token_id', 0), sparse=False)\n\n        encoder_layer = nn.TransformerEncoderLayer(d_model=config.field_hidden_size, nhead=config.nhead,\n                                                   dim_feedforward=config.field_hidden_size)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n\n        self.lin_proj = nn.Linear(config.field_hidden_size * config.ncols, config.hidden_size)\n\n    def forward(self, input_ids):\n        inputs_embeds = self.word_embeddings(input_ids)\n        embeds_shape = list(inputs_embeds.size())\n\n        inputs_embeds = inputs_embeds.view([-1] + embeds_shape[-2:])\n        inputs_embeds = inputs_embeds.permute(1, 0, 2)\n        inputs_embeds = self.transformer_encoder(inputs_embeds)\n        inputs_embeds = inputs_embeds.permute(1, 0, 2)\n        inputs_embeds = inputs_embeds.contiguous().view(embeds_shape[0:2]+[-1])\n\n        inputs_embeds = self.lin_proj(inputs_embeds)\n\n        return inputs_embeds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. TabFormer BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n\n\nclass TabFormerBertConfig(BertConfig):\n    def __init__(\n        self,\n        flatten=True,\n        ncols=12,\n        vocab_size=30522,\n        field_hidden_size=64,\n        hidden_size=768,\n        num_attention_heads=12,\n        pad_token_id=0,\n        **kwargs\n    ):\n        super().__init__(pad_token_id=pad_token_id, **kwargs)\n\n        self.ncols = ncols\n        self.field_hidden_size = field_hidden_size\n        self.hidden_size = hidden_size\n        self.flatten = flatten\n        self.vocab_size = vocab_size\n        self.num_attention_heads=num_attention_heads\n\nclass TabFormerBertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.field_hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\nclass TabFormerBertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.transform = TabFormerBertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n        self.decoder.bias = self.bias\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states)\n        return hidden_states\n\nclass TabFormerBertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = TabFormerBertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\nclass TabFormerBertForMaskedLM(BertForMaskedLM):\n    def __init__(self, config, vocab):\n        super().__init__(config)\n\n        self.vocab = vocab\n        self.cls = TabFormerBertOnlyMLMHead(config)\n        self.init_weights()\n\n    def forward(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            token_type_ids=None,\n            position_ids=None,\n            head_mask=None,\n            inputs_embeds=None,\n            masked_lm_labels=None,\n            encoder_hidden_states=None,\n            encoder_attention_mask=None,\n            lm_labels=None,\n    ):\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n        )\n\n        sequence_output = outputs[0]  # [bsz * seqlen * hidden]\n\n        if not self.config.flatten:\n            output_sz = list(sequence_output.size())\n            expected_sz = [output_sz[0], output_sz[1]*self.config.ncols, -1]\n            sequence_output = sequence_output.view(expected_sz)\n            masked_lm_labels = masked_lm_labels.view(expected_sz[0], -1)\n\n        prediction_scores = self.cls(sequence_output) # [bsz * seqlen * vocab_sz]\n\n        outputs = (prediction_scores,) + outputs[2:]\n\n        # prediction_scores : [bsz x seqlen x vsz]\n        # masked_lm_labels  : [bsz x seqlen]\n\n        total_masked_lm_loss = 0\n\n        seq_len = prediction_scores.size(1)\n        # TODO : remove_target is True for card\n        field_names = self.vocab.get_field_keys(remove_target=True, ignore_special=False)\n        for field_idx, field_name in enumerate(field_names):\n            col_ids = list(range(field_idx, seq_len, len(field_names)))\n\n            global_ids_field = self.vocab.get_field_ids(field_name)\n\n            prediction_scores_field = prediction_scores[:, col_ids, :][:, :, global_ids_field]  # bsz * 10 * K\n            masked_lm_labels_field = masked_lm_labels[:, col_ids]\n            masked_lm_labels_field_local = self.vocab.get_from_global_ids(global_ids=masked_lm_labels_field,\n                                                                          what_to_get='local_ids')\n\n            nfeas = len(global_ids_field)\n            loss_fct = self.get_criterion(field_name, nfeas, prediction_scores.device)\n\n            masked_lm_loss_field = loss_fct(prediction_scores_field.view(-1, len(global_ids_field)),\n                                            masked_lm_labels_field_local.view(-1))\n\n            total_masked_lm_loss += masked_lm_loss_field\n\n        return (total_masked_lm_loss,) + outputs\n\n    def get_criterion(self, fname, vs, device, cutoffs=False, div_value=4.0):\n\n        if fname in self.vocab.adap_sm_cols:\n            if not cutoffs:\n                cutoffs = [int(vs/15), 3*int(vs/15), 6*int(vs/15)]\n\n            criteria = CustomAdaptiveLogSoftmax(in_features=vs, n_classes=vs, cutoffs=cutoffs, div_value=div_value)\n\n            return criteria.to(device)\n        else:\n            return CrossEntropyLoss()\n\nclass TabFormerBertModel(BertForMaskedLM):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.cls = TabFormerBertOnlyMLMHead(config)\n        self.init_weights()\n\n    def forward(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            token_type_ids=None,\n            position_ids=None,\n            head_mask=None,\n            inputs_embeds=None,\n            masked_lm_labels=None,\n            encoder_hidden_states=None,\n            encoder_attention_mask=None,\n            lm_labels=None,\n    ):\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n        )\n\n        sequence_output = outputs[0]  # [bsz * seqlen * hidden]\n\n        return sequence_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. TabFormer GPT2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n\n\nclass TabFormerGPT2LMHeadModel(GPT2LMHeadModel):\n    def __init__(self, config, vocab):\n        super().__init__(config)\n        self.vocab = vocab\n\n    def forward(\n            self,\n            input_ids=None,\n            past=None,\n            attention_mask=None,\n            token_type_ids=None,\n            position_ids=None,\n            head_mask=None,\n            inputs_embeds=None,\n            labels=None,\n            use_cache=True,\n    ):\n        transformer_outputs = self.transformer(\n            input_ids,\n            past=past,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n        )\n        hidden_states = transformer_outputs[0]\n        lm_logits = self.lm_head(hidden_states)\n\n        # lm_logits : [bsz x seq_len x vsz]\n        # labels    : [bsz x seq_len]\n        # When flatten is set to True:\n        # seq_len = num_transactions * (num_columns + 2)  --> plus 2 because each transaction has BOS and EOS padding\n\n        outputs = (lm_logits,) + transformer_outputs[1:]\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_labels = labels[..., 1:-1].contiguous()  # Remove first and last label: [BOS] and [EOS] tokens\n            shift_logits = lm_logits[..., :-2, :].contiguous()  # Line up logits accordingly\n\n            seq_len = shift_logits.size(1)\n            total_lm_loss = 0\n            field_names = self.vocab.get_field_keys(remove_target=True, ignore_special=True)\n\n            for field_idx, field_name in enumerate(field_names):\n                col_ids = list(range(field_idx, seq_len, len(field_names)))\n                global_ids_field = self.vocab.get_field_ids(field_name)\n                lm_logits_field = shift_logits[:, col_ids, :][:, :, global_ids_field]  # bsz * 10 * K\n                lm_labels_field = shift_labels[:, col_ids]\n                lm_labels_local_field = self.vocab.get_from_global_ids(global_ids=lm_labels_field,\n                                                                       what_to_get='local_ids')\n\n                loss_fct = CrossEntropyLoss()\n                lm_loss_field = loss_fct(lm_logits_field.view(-1, len(global_ids_field)),\n                                         lm_labels_local_field.view(-1))\n                total_lm_loss += lm_loss_field\n\n            outputs = (total_lm_loss,) + outputs\n\n        return outputs  # (loss), lm_logits, presents, (all hidden_states), (attentions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n    BertTokenizer,\n    BertForMaskedLM,\n    GPT2Config,\n    GPT2LMHeadModel\n)\n\n\n\nclass TabFormerBaseModel(PreTrainedModel):\n    def __init__(self, hf_model, tab_embeddings, config):\n        super().__init__(config)\n\n        self.model = hf_model\n        self.tab_embeddings = tab_embeddings\n\n    def forward(self, input_ids, **input_args):\n        inputs_embeds = self.tab_embeddings(input_ids)\n        return self.model(inputs_embeds=inputs_embeds, **input_args)\n\n\nclass TabFormerHierarchicalLM(PreTrainedModel):\n    base_model_prefix = \"bert\"\n\n    def __init__(self, config, vocab):\n        super().__init__(config)\n\n        self.config = config\n\n        self.tab_embeddings = TabFormerEmbeddings(self.config)\n        self.tb_model = TabFormerBertForMaskedLM(self.config, vocab)\n\n    def forward(self, input_ids, **input_args):\n        inputs_embeds = self.tab_embeddings(input_ids)\n        return self.tb_model(inputs_embeds=inputs_embeds, **input_args)\n\n\nclass TabFormerBertLM:\n    def __init__(self, special_tokens, vocab, field_ce=False, flatten=False, ncols=None, field_hidden_size=768):\n\n        self.ncols = ncols\n        self.vocab = vocab\n        vocab_file = self.vocab.filename\n        hidden_size = field_hidden_size if flatten else (field_hidden_size * self.ncols)\n\n        self.config = TabFormerBertConfig(vocab_size=len(self.vocab),\n                                          ncols=self.ncols,\n                                          hidden_size=hidden_size,\n                                          field_hidden_size=field_hidden_size,\n                                          flatten=flatten,\n                                          num_attention_heads=self.ncols)\n\n        self.tokenizer = BertTokenizer(vocab_file,\n                                       do_lower_case=False,\n                                       **special_tokens)\n        self.model = self.get_model(field_ce, flatten)\n\n    def get_model(self, field_ce, flatten):\n\n        if flatten and not field_ce:\n            # flattened vanilla BERT\n            model = BertForMaskedLM(self.config)\n        elif flatten and field_ce:\n            # flattened field CE BERT\n            model = TabFormerBertForMaskedLM(self.config, self.vocab)\n        else:\n            # hierarchical field CE BERT\n            model = TabFormerHierarchicalLM(self.config, self.vocab)\n\n        return model\n\n\nclass TabFormerGPT2:\n    def __init__(self, special_tokens, vocab, field_ce=False, flatten=False):\n\n        self.vocab = vocab\n        self.config = GPT2Config(vocab_size=len(self.vocab))\n\n        self.tokenizer = TabFormerTokenizer(\n            unk_token=special_tokens.unk_token,\n            bos_token=special_tokens.bos_token,\n            eos_token=special_tokens.eos_token\n        )\n\n        self.model = self.get_model(field_ce, flatten)\n\n    def get_model(self, field_ce, flatten):\n        if field_ce:\n            model = TabFormerGPT2LMHeadModel(self.config, self.vocab)\n        else:\n            model = GPT2LMHeadModel(self.config)\n        if not flatten:\n            tab_emb_config = ddict(vocab_size=len(self.vocab), hidden_size=self.config.hidden_size)\n            model = TabFormerBaseModel(model, TabFormerEmbeddings(tab_emb_config))\n\n        return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Data Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n\nclass TransDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n\n    def __call__(\n            self, examples: List[Union[List[int], torch.Tensor, Dict[str, torch.Tensor]]]\n    ) -> Dict[str, torch.Tensor]:\n        batch = self._tensorize_batch(examples)\n        sz = batch.shape\n        if self.mlm:\n            batch = batch.view(sz[0], -1)\n            inputs, labels = self.mask_tokens(batch)\n            return {\"input_ids\": inputs.view(sz), \"masked_lm_labels\": labels.view(sz)}\n        else:\n            labels = batch.clone().detach()\n            if self.tokenizer.pad_token_id is not None:\n                labels[labels == self.tokenizer.pad_token_id] = -100\n            return {\"input_ids\": batch, \"labels\": labels}\n\n    def mask_tokens(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n        \"\"\"\n        if self.tokenizer.mask_token is None:\n            raise ValueError(\n                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove \"\n                \"the --mlm flag if you want to use this tokenizer. \"\n            )\n        labels = inputs.clone()\n        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability\n        # defaults to 0.15 in Bert/RoBERTa)\n        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n        special_tokens_mask = [\n            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n        ]\n        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n        if self.tokenizer._pad_token is not None:\n            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n            probability_matrix.masked_fill_(padding_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Transaction Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n\n\n\n\n\nclass TransactionDataset(Dataset):\n    def __init__(self,\n                 mlm,\n                 user_ids=None,\n                 seq_len=10,\n                 num_bins=10,\n                 cached=True,\n                 root=\"./data/card/\",\n                 fname=\"card_trans\",\n                 vocab_dir=\"checkpoints\",\n                 fextension=\"\",\n                 nrows=None,\n                 flatten=False,\n                 stride=5,\n                 adap_thres=10 ** 8,\n                 return_labels=False,\n                 skip_user=False):\n\n        self.root = root\n        self.fname = fname\n        self.nrows = nrows\n        self.fextension = f'_{fextension}' if fextension else ''\n        self.cached = cached\n        self.user_ids = user_ids\n        self.return_labels = return_labels\n        self.skip_user = skip_user\n\n        self.mlm = mlm\n        self.trans_stride = stride\n\n        self.flatten = flatten\n\n        self.vocab = Vocabulary(adap_thres)\n        self.seq_len = seq_len\n        self.encoder_fit = {}\n\n        self.trans_table = None\n        self.data = []\n        self.labels = []\n        self.window_label = []\n\n        self.ncols = None\n        self.num_bins = num_bins\n        self.encode_data()\n        self.init_vocab()\n        self.prepare_samples()\n        self.save_vocab(vocab_dir)\n\n    def __getitem__(self, index):\n        if self.flatten:\n            return_data = torch.tensor(self.data[index], dtype=torch.long)\n        else:\n            return_data = torch.tensor(self.data[index], dtype=torch.long).reshape(self.seq_len, -1)\n\n        if self.return_labels:\n            return_data = (return_data, torch.tensor(self.labels[index], dtype=torch.long))\n\n        return return_data\n\n    def __len__(self):\n        return len(self.data)\n\n    def save_vocab(self, vocab_dir):\n        file_name = path.join(vocab_dir, f'vocab{self.fextension}.nb')\n        log.info(f\"saving vocab at {file_name}\")\n        self.vocab.save_vocab(file_name)\n\n    @staticmethod\n    def label_fit_transform(column, enc_type=\"label\"):\n        if enc_type == \"label\":\n            mfit = LabelEncoder()\n        else:\n            mfit = MinMaxScaler()\n        mfit.fit(column)\n\n        return mfit, mfit.transform(column)\n\n    @staticmethod\n    def timeEncoder(X):\n        X_hm = X['Time'].str.split(':', expand=True)\n        d = pd.to_datetime(dict(year=X['Year'], month=X['Month'], day=X['Day'], hour=X_hm[0], minute=X_hm[1])).astype(\n            int)\n        return pd.DataFrame(d)\n\n    @staticmethod\n    def amountEncoder(X):\n        amt = X.apply(lambda x: x[1:]).astype(float).apply(lambda amt: max(1, amt)).apply(math.log)\n        return pd.DataFrame(amt)\n\n    @staticmethod\n    def fraudEncoder(X):\n        fraud = (X == 'Yes').astype(int)\n        return pd.DataFrame(fraud)\n\n    @staticmethod\n    def nanNone(X):\n        return X.where(pd.notnull(X), 'None')\n\n    @staticmethod\n    def nanZero(X):\n        return X.where(pd.notnull(X), 0)\n\n    def _quantization_binning(self, data):\n        qtls = np.arange(0.0, 1.0 + 1 / self.num_bins, 1 / self.num_bins)\n        bin_edges = np.quantile(data, qtls, axis=0)  # (num_bins + 1, num_features)\n        bin_widths = np.diff(bin_edges, axis=0)\n        bin_centers = bin_edges[:-1] + bin_widths / 2  # ()\n        return bin_edges, bin_centers, bin_widths\n\n    def _quantize(self, inputs, bin_edges):\n        quant_inputs = np.zeros(inputs.shape[0])\n        for i, x in enumerate(inputs):\n            quant_inputs[i] = np.digitize(x, bin_edges)\n        quant_inputs = quant_inputs.clip(1, self.num_bins) - 1  # Clip edges\n        return quant_inputs\n\n    def user_level_data(self):\n        fname = path.join(self.root, f\"preprocessed/{self.fname}.user{self.fextension}.pkl\")\n        trans_data, trans_labels = [], []\n\n        if self.cached and path.isfile(fname):\n            log.info(f\"loading cached user level data from {fname}\")\n            cached_data = pickle.load(open(fname, \"rb\"))\n            trans_data = cached_data[\"trans\"]\n            trans_labels = cached_data[\"labels\"]\n            columns_names = cached_data[\"columns\"]\n\n        else:\n            unique_users = self.trans_table[\"User\"].unique()\n            columns_names = list(self.trans_table.columns)\n\n            for user in tqdm.tqdm(unique_users):\n                user_data = self.trans_table.loc[self.trans_table[\"User\"] == user]\n                user_trans, user_labels = [], []\n                for idx, row in user_data.iterrows():\n                    row = list(row)\n\n                    # assumption that user is first field\n                    skip_idx = 1 if self.skip_user else 0\n\n                    user_trans.extend(row[skip_idx:-1])\n                    user_labels.append(row[-1])\n\n                trans_data.append(user_trans)\n                trans_labels.append(user_labels)\n\n            if self.skip_user:\n                columns_names.remove(\"User\")\n\n            with open(fname, 'wb') as cache_file:\n                pickle.dump({\"trans\": trans_data, \"labels\": trans_labels, \"columns\": columns_names}, cache_file)\n\n        # convert to str\n        return trans_data, trans_labels, columns_names\n\n    def format_trans(self, trans_lst, column_names):\n        trans_lst = list(divide_chunks(trans_lst, len(self.vocab.field_keys) - 2))  # 2 to ignore isFraud and SPECIAL\n        user_vocab_ids = []\n\n        sep_id = self.vocab.get_id(self.vocab.sep_token, special_token=True)\n\n        for trans in trans_lst:\n            vocab_ids = []\n            for jdx, field in enumerate(trans):\n                vocab_id = self.vocab.get_id(field, column_names[jdx])\n                vocab_ids.append(vocab_id)\n\n            # TODO : need to handle ncols when sep is not added\n            if self.mlm:  # and self.flatten:  # only add [SEP] for BERT + flatten scenario\n                vocab_ids.append(sep_id)\n\n            user_vocab_ids.append(vocab_ids)\n\n        return user_vocab_ids\n\n    def prepare_samples(self):\n        log.info(\"preparing user level data...\")\n        trans_data, trans_labels, columns_names = self.user_level_data()\n\n        log.info(\"creating transaction samples with vocab\")\n        for user_idx in tqdm.tqdm(range(len(trans_data))):\n            user_row = trans_data[user_idx]\n            user_row_ids = self.format_trans(user_row, columns_names)\n\n            user_labels = trans_labels[user_idx]\n\n            bos_token = self.vocab.get_id(self.vocab.bos_token, special_token=True)  # will be used for GPT2\n            eos_token = self.vocab.get_id(self.vocab.eos_token, special_token=True)  # will be used for GPT2\n            for jdx in range(0, len(user_row_ids) - self.seq_len + 1, self.trans_stride):\n                ids = user_row_ids[jdx:(jdx + self.seq_len)]\n                ids = [idx for ids_lst in ids for idx in ids_lst]  # flattening\n                if not self.mlm and self.flatten:  # for GPT2, need to add [BOS] and [EOS] tokens\n                    ids = [bos_token] + ids + [eos_token]\n                self.data.append(ids)\n\n            for jdx in range(0, len(user_labels) - self.seq_len + 1, self.trans_stride):\n                ids = user_labels[jdx:(jdx + self.seq_len)]\n                self.labels.append(ids)\n\n                fraud = 0\n                if len(np.nonzero(ids)[0]) > 0:\n                    fraud = 1\n                self.window_label.append(fraud)\n\n        assert len(self.data) == len(self.labels)\n\n        '''\n            ncols = total fields - 1 (special tokens) - 1 (label)\n            if bert:\n                ncols += 1 (for sep)\n        '''\n        self.ncols = len(self.vocab.field_keys) - 2 + (1 if self.mlm else 0)\n        log.info(f\"ncols: {self.ncols}\")\n        log.info(f\"no of samples {len(self.data)}\")\n\n    def get_csv(self, fname):\n        data = pd.read_csv(fname, nrows=self.nrows)\n        if self.user_ids:\n            log.info(f'Filtering data by user ids list: {self.user_ids}...')\n            self.user_ids = map(int, self.user_ids)\n            data = data[data['User'].isin(self.user_ids)]\n\n        self.nrows = data.shape[0]\n        log.info(f\"read data : {data.shape}\")\n        return data\n\n    def write_csv(self, data, fname):\n        log.info(f\"writing to file {fname}\")\n        data.to_csv(fname, index=False)\n\n    def init_vocab(self):\n        column_names = list(self.trans_table.columns)\n        if self.skip_user:\n            column_names.remove(\"User\")\n\n        self.vocab.set_field_keys(column_names)\n\n        for column in column_names:\n            unique_values = self.trans_table[column].value_counts(sort=True).to_dict()  # returns sorted\n            for val in unique_values:\n                self.vocab.set_id(val, column)\n\n        log.info(f\"total columns: {list(column_names)}\")\n        log.info(f\"total vocabulary size: {len(self.vocab.id2token)}\")\n\n        for column in self.vocab.field_keys:\n            vocab_size = len(self.vocab.token2id[column])\n            log.info(f\"column : {column}, vocab size : {vocab_size}\")\n\n            if vocab_size > self.vocab.adap_thres:\n                log.info(f\"\\tsetting {column} for adaptive softmax\")\n                self.vocab.adap_sm_cols.add(column)\n\n    def encode_data(self):\n        dirname = path.join(self.root, \"preprocessed\")\n        fname = f'{self.fname}{self.fextension}.encoded.csv'\n        data_file = path.join(self.root, f\"{self.fname}.csv\")\n\n        if self.cached and path.isfile(path.join(dirname, fname)):\n            log.info(f\"cached encoded data is read from {fname}\")\n            self.trans_table = self.get_csv(path.join(dirname, fname))\n            encoder_fname = path.join(dirname, f'{self.fname}{self.fextension}.encoder_fit.pkl')\n            self.encoder_fit = pickle.load(open(encoder_fname, \"rb\"))\n            return\n\n        data = self.get_csv(data_file)\n        log.info(f\"{data_file} is read.\")\n\n        log.info(\"nan resolution.\")\n        data['Errors?'] = self.nanNone(data['Errors?'])\n        data['Is Fraud?'] = self.fraudEncoder(data['Is Fraud?'])\n        data['Zip'] = self.nanZero(data['Zip'])\n        data['Merchant State'] = self.nanNone(data['Merchant State'])\n        data['Use Chip'] = self.nanNone(data['Use Chip'])\n        data['Amount'] = self.amountEncoder(data['Amount'])\n\n        sub_columns = ['Errors?', 'MCC', 'Zip', 'Merchant State', 'Merchant City', 'Merchant Name', 'Use Chip']\n\n        log.info(\"label-fit-transform.\")\n        for col_name in tqdm.tqdm(sub_columns):\n            col_data = data[col_name]\n            col_fit, col_data = self.label_fit_transform(col_data)\n            self.encoder_fit[col_name] = col_fit\n            data[col_name] = col_data\n\n        log.info(\"timestamp fit transform\")\n        timestamp = self.timeEncoder(data[['Year', 'Month', 'Day', 'Time']])\n        timestamp_fit, timestamp = self.label_fit_transform(timestamp, enc_type=\"time\")\n        self.encoder_fit['Timestamp'] = timestamp_fit\n        data['Timestamp'] = timestamp\n\n        log.info(\"timestamp quant transform\")\n        coldata = np.array(data['Timestamp'])\n        bin_edges, bin_centers, bin_widths = self._quantization_binning(coldata)\n        data['Timestamp'] = self._quantize(coldata, bin_edges)\n        self.encoder_fit[\"Timestamp-Quant\"] = [bin_edges, bin_centers, bin_widths]\n\n        log.info(\"amount quant transform\")\n        coldata = np.array(data['Amount'])\n        bin_edges, bin_centers, bin_widths = self._quantization_binning(coldata)\n        data['Amount'] = self._quantize(coldata, bin_edges)\n        self.encoder_fit[\"Amount-Quant\"] = [bin_edges, bin_centers, bin_widths]\n\n        columns_to_select = ['User',\n                             'Card',\n                             'Timestamp',\n                             'Amount',\n                             'Use Chip',\n                             'Merchant Name',\n                             'Merchant City',\n                             'Merchant State',\n                             'Zip',\n                             'MCC',\n                             'Errors?',\n                             'Is Fraud?']\n\n        self.trans_table = data[columns_to_select]\n\n        log.info(f\"writing cached csv to {path.join(dirname, fname)}\")\n        if not path.exists(dirname):\n            os.mkdir(dirname)\n        self.write_csv(self.trans_table, path.join(dirname, fname))\n\n        encoder_fname = path.join(dirname, f'{self.fname}{self.fextension}.encoder_fit.pkl')\n        log.info(f\"writing cached encoder fit to {encoder_fname}\")\n        pickle.dump(self.encoder_fit, open(encoder_fname, \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Configuration for Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "class Config:\n",
        "    # Paths - Kaggle specific\n",
        "    data_root = \"/kaggle/input/\"\n",
        "    output_dir = \"/kaggle/working/checkpoints\"\n",
        "    log_dir = \"/kaggle/working/logs\"\n",
        "    \n",
        "    # Data\n",
        "    data_type = \"card\"\n",
        "    data_fname = \"card_transaction.v1\"\n",
        "    data_extension = \"\"\n",
        "    nrows = 10000  # Limit for faster training, set to None for all\n",
        "    cached = False\n",
        "    user_ids = None\n",
        "    skip_user = False\n",
        "    \n",
        "    # Model\n",
        "    lm_type = \"bert\"\n",
        "    mlm = True\n",
        "    mlm_prob = 0.15\n",
        "    field_ce = True\n",
        "    flatten = False\n",
        "    field_hs = 64\n",
        "    \n",
        "    # Training\n",
        "    do_train = True\n",
        "    num_train_epochs = 3\n",
        "    save_steps = 500\n",
        "    stride = 5\n",
        "    checkpoint = 0\n",
        "    seed = 9\n",
        "\n",
        "args = Config()\n",
        "makedirs(args.output_dir, exist_ok=True)\n",
        "makedirs(args.log_dir, exist_ok=True)\n",
        "\n",
        "print(\"\u2713 Configuration set\")\n",
        "print(f\"Data: {args.data_root}\")\n",
        "print(f\"Output: {args.output_dir}\")\n",
        "print(f\"Model: {args.lm_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Main Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "    # Set seeds\n",
        "    seed = args.seed\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    # Load dataset\n",
        "    log.info(\"Loading dataset...\")\n",
        "    if args.data_type == 'card':\n",
        "        dataset = TransactionDataset(\n",
        "            root=args.data_root,\n",
        "            fname=args.data_fname,\n",
        "            fextension=args.data_extension,\n",
        "            vocab_dir=args.output_dir,\n",
        "            nrows=args.nrows,\n",
        "            user_ids=args.user_ids,\n",
        "            mlm=args.mlm,\n",
        "            cached=args.cached,\n",
        "            stride=args.stride,\n",
        "            flatten=args.flatten,\n",
        "            return_labels=False,\n",
        "            skip_user=args.skip_user\n",
        "        )\n",
        "    else:\n",
        "        raise Exception(f\"Data type '{args.data_type}' not defined\")\n",
        "    \n",
        "    vocab = dataset.vocab\n",
        "    custom_special_tokens = vocab.get_special_tokens()\n",
        "    \n",
        "    # Split dataset\n",
        "    totalN = len(dataset)\n",
        "    trainN = int(0.6 * totalN)\n",
        "    valtestN = totalN - trainN\n",
        "    valN = int(valtestN * 0.5)\n",
        "    testN = valtestN - valN\n",
        "    lengths = [trainN, valN, testN]\n",
        "    \n",
        "    log.info(f\"Sizes - Train: {trainN}, Val: {valN}, Test: {testN}\")\n",
        "    train_dataset, eval_dataset, test_dataset = random_split_dataset(dataset, lengths)\n",
        "    \n",
        "    # Initialize model\n",
        "    log.info(\"Initializing model...\")\n",
        "    if args.lm_type == \"bert\":\n",
        "        tab_net = TabFormerBertLM(\n",
        "            custom_special_tokens,\n",
        "            vocab=vocab,\n",
        "            field_ce=args.field_ce,\n",
        "            flatten=args.flatten,\n",
        "            ncols=dataset.ncols,\n",
        "            field_hidden_size=args.field_hs\n",
        "        )\n",
        "    else:\n",
        "        tab_net = TabFormerGPT2(\n",
        "            custom_special_tokens,\n",
        "            vocab=vocab,\n",
        "            field_ce=args.field_ce,\n",
        "            flatten=args.flatten\n",
        "        )\n",
        "    \n",
        "    log.info(f\"Model: {tab_net.model.__class__}\")\n",
        "    \n",
        "    # Data collator\n",
        "    if args.flatten:\n",
        "        collator_cls = DataCollatorForLanguageModeling\n",
        "    else:\n",
        "        collator_cls = TransDataCollatorForLanguageModeling\n",
        "    \n",
        "    data_collator = collator_cls(\n",
        "        tokenizer=tab_net.tokenizer,\n",
        "        mlm=args.mlm,\n",
        "        mlm_probability=args.mlm_prob\n",
        "    )\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        num_train_epochs=args.num_train_epochs,\n",
        "        logging_dir=args.log_dir,\n",
        "        save_steps=args.save_steps,\n",
        "        do_train=args.do_train,\n",
        "        prediction_loss_only=True,\n",
        "        overwrite_output_dir=True,\n",
        "        use_cpu=True,\n",
        "        no_cuda=True\n",
        "    )\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=tab_net.model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    log.info(\"Starting training...\")\n",
        "    model_path = None\n",
        "    if args.checkpoint:\n",
        "        model_path = path.join(args.output_dir, f'checkpoint-{args.checkpoint}')\n",
        "    \n",
        "    trainer.train(resume_from_checkpoint=model_path)\n",
        "    log.info(\"Training complete!\")\n",
        "    \n",
        "    # Save model\n",
        "    trainer.save_model(path.join(args.output_dir, \"final_model\"))\n",
        "    log.info(f\"Model saved to {args.output_dir}/final_model\")\n",
        "    \n",
        "    return trainer\n",
        "\n",
        "print(\"\u2713 Training function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Run Training\n",
        "\n",
        "**Important:** Ensure your CSV data is in `/kaggle/input/` with correct column names:\n",
        "`User, Card, Year, Month, Day, Time, Amount, Use Chip, Merchant Name, ",
        "Merchant City, Merchant State, Zip, MCC, Errors?, Is Fraud?`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate config\n",
        "if args.mlm and args.lm_type == \"gpt2\":\n",
        "    raise Exception(\"GPT2 doesn't need MLM\")\n",
        "if not args.mlm and args.lm_type == \"bert\":\n",
        "    raise Exception(\"BERT needs MLM\")\n",
        "\n",
        "# Display config\n",
        "display(HTML(\"<h3>Training Configuration</h3>\"))\n",
        "config_html = f\"\"\"\n",
        "<table border='1'>\n",
        "<tr><td><b>Model</b></td><td>{args.lm_type}</td></tr>\n",
        "<tr><td><b>MLM</b></td><td>{args.mlm}</td></tr>\n",
        "<tr><td><b>Field CE</b></td><td>{args.field_ce}</td></tr>\n",
        "<tr><td><b>Epochs</b></td><td>{args.num_train_epochs}</td></tr>\n",
        "<tr><td><b>Rows</b></td><td>{args.nrows or 'All'}</td></tr>\n",
        "</table>\n",
        "\"\"\"\n",
        "display(HTML(config_html))\n",
        "\n",
        "# Run\n",
        "try:\n",
        "    trainer = main(args)\n",
        "    display(HTML(\"<h3 style='color:green'>\u2713 Training completed!</h3>\"))\n",
        "except Exception as e:\n",
        "    display(HTML(f\"<h3 style='color:red'>\u2717 Error: {str(e)}</h3>\"))\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List outputs\n",
        "print(\"Output files:\")\n",
        "for root, dirs, files in os.walk(args.output_dir):\n",
        "    level = root.replace(args.output_dir, '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files[:10]:\n",
        "        print(f\"{subindent}{file}\")\n",
        "    if len(files) > 10:\n",
        "        print(f\"{subindent}... {len(files)-10} more\")\n",
        "\n",
        "print(\"\\n\u2713 Check /kaggle/working/ for all outputs!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}